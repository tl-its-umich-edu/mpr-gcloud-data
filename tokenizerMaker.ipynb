{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9559b7df-d9eb-4617-b546-254f75c8f1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieiving tokenizer builder text files...\n",
      "Tokenizer builder retrieved.\n",
      "Building tokenizer for vocabulary size 30522...\n",
      "\n",
      "\n",
      "\n",
      "Saved model files in gs://mpr-research-tokenizers/30522\n",
      "Building tokenizer for vocabulary size 50265...\n",
      "\n",
      "\n",
      "\n",
      "Saved model files in gs://mpr-research-tokenizers/50265\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from google.cloud import storage\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "tokenizerBucketName = 'mpr-research-tokenizers'\n",
    "modelBucketName = 'mwrite-data-bucket-1'\n",
    "\n",
    "client = storage.Client()\n",
    "\n",
    "tokenizerParentFolder = 'peerBERT_tokenizers'\n",
    "if not os.path.exists(tokenizerParentFolder):\n",
    "    os.makedirs(tokenizerParentFolder)\n",
    "    \n",
    "    \n",
    "print('Retrieiving tokenizer builder text files...')\n",
    "tokenizerTrainerDataFolder = 'PeerBERT_training_text'\n",
    "if not os.path.exists(tokenizerTrainerDataFolder):\n",
    "    os.makedirs(tokenizerTrainerDataFolder)\n",
    "    \n",
    "fileBucket = client.get_bucket((modelBucketName))\n",
    "\n",
    "for file in fileBucket.list_blobs(prefix=tokenizerTrainerDataFolder):\n",
    "    try:\n",
    "        file.download_to_filename(file.name)\n",
    "    except:\n",
    "        continue\n",
    "print('Tokenizer builder retrieved.')\n",
    "    \n",
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def tokenizerMaker(vocabularySize):\n",
    "    \n",
    "    print(f'Building tokenizer for vocabulary size {vocabularySize}...')\n",
    "    \n",
    "    tokenizerFolder = str(vocabularySize)\n",
    "    if not os.path.exists(os.path.join(tokenizerParentFolder,tokenizerFolder)):\n",
    "        os.makedirs(os.path.join(tokenizerParentFolder,tokenizerFolder))\n",
    "        \n",
    "    baseTokenizer = ByteLevelBPETokenizer(lowercase=False)\n",
    "\n",
    "    pathList = [os.path.join(tokenizerTrainerDataFolder,file) for file in os.listdir(tokenizerTrainerDataFolder)]\n",
    "\n",
    "    baseTokenizer.train(files=pathList, vocab_size=vocabularySize, min_frequency=2, special_tokens=[\n",
    "        \"<s>\",\n",
    "        \"<pad>\",\n",
    "        \"</s>\",\n",
    "        \"<unk>\",\n",
    "        \"<mask>\",\n",
    "        ] )\n",
    "\n",
    "    baseTokenizer.save_model(os.path.join(tokenizerParentFolder,tokenizerFolder))\n",
    "    \n",
    "    try:\n",
    "        tokenizerBucket = client.bucket(tokenizerBucketName)\n",
    "        localTokenizerPath = os.path.join(tokenizerParentFolder,tokenizerFolder)\n",
    "        files = [f for f in os.listdir(localTokenizerPath) if os.path.isfile(os.path.join(localTokenizerPath, f))]\n",
    "\n",
    "        for file in files:\n",
    "            localFile = os.path.join(localTokenizerPath, file)\n",
    "            blob = tokenizerBucket.blob(\"/\".join([tokenizerFolder, file]))\n",
    "            blob.upload_from_filename(localFile)\n",
    "        print(f'Saved model files in gs://{tokenizerBucketName}/{tokenizerFolder}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error: {e}')\n",
    "        print(f'Saved tokenizer files instead locally at {os.path.join(tokenizerParentFolder,tokenizerFolder)}.')\n",
    "\n",
    "    \n",
    "\n",
    "for vocabularySize in [30522, 50265]:\n",
    "    tokenizerMaker(vocabularySize)\n",
    "    \n",
    "shutil.rmtree(tokenizerParentFolder)\n",
    "shutil.rmtree(tokenizerTrainerDataFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd59cbf-cf32-4cbc-8228-22b397f46abe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
