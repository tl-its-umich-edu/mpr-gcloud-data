{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "104a3fcf-b0fa-438f-8610-34967f8a457d",
   "metadata": {},
   "source": [
    "# peerBERT Trainer Notebook \n",
    "\n",
    "The following are the libraries we need to install, assuming that we started from a default Python Jupyter Notebook.\n",
    "Comment them out once you have installed them the first time. User-managed notebooks are persistent and do not require installing libraries every run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03df7c5f-b5e6-43b7-b1ec-721981387456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers --upgrade\n",
    "# !pip install datasets --upgrade\n",
    "# !pip install torch --upgrade\n",
    "# !pip install openpyxl --upgrade\n",
    "# !pip install accelerate --upgrade\n",
    "# !pip install 'ray[tune]'\n",
    "# !pip install tensorboard\n",
    "# !pip install cloudml-hypertune\n",
    "\n",
    "# !pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041a8656-10e1-40e5-9585-c69ae4266187",
   "metadata": {},
   "source": [
    "We start by importing the required libraries and then setting up our classes and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d9326e2-e028-4937-b9e7-cb30861c9b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "from datasets import load_metric, Metric\n",
    "from transformers import (RobertaConfig, RobertaTokenizer, RobertaForSequenceClassification,\n",
    "                        TrainerCallback, EarlyStoppingCallback, TrainingArguments, Trainer)\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, SequentialSampler\n",
    "import hypertune\n",
    "from sklearn.metrics import hamming_loss, confusion_matrix, ConfusionMatrixDisplay, f1_score, matthews_corrcoef\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use for debugging to ger verbose messages if something goes wrong on a CUDA level\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)-4s [%(filename)s:%(lineno)d] - %(message)s',\n",
    "    datefmt='%Y-%m-%dT%H:%M:%S%z',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# The config Class contains all the information needed for running the Trainer, it can be also confifured using command line arguments.\n",
    "# Command line arguments do not work in Jupyter notebooks, so they're ignored in the notebook.\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.logLevel = logging.INFO\n",
    "        self.vocabularySize: int = 50265\n",
    "        self.hiddenLayers: int = 6\n",
    "        self.tierLevel: int = 1\n",
    "        self.batchSize: int = 16\n",
    "        self.metricType: str = 'matthews_correlation'\n",
    "        self.numEpochs: int = 25\n",
    "        self.earlyStopping: int = 3\n",
    "        self.hpTune: bool = True\n",
    "        \n",
    "        self.tokenizerBucketName: str = 'mpr-research-tokenizers'\n",
    "        self.modelBucketName: str = 'mpr-research-models'\n",
    "        self.dataBucketName: str = 'mwrite-data-bucket-1'\n",
    "        \n",
    "        self.tempDownloadFolder: str = './tmp'\n",
    "        if not os.path.exists(self.tempDownloadFolder):\n",
    "            os.makedirs(self.tempDownloadFolder)\n",
    "            \n",
    "        self.problemTypeDict = {1:'single_label_classification' ,2:'multi_label_classification' }\n",
    "\n",
    "        self.labelTierDict = {1:['Verification/Summary', 'Praise', 'Problem/Solution'],\n",
    "                              2:['Writing/Formatting \\nIssues', 'Missing Content', 'Incorrect Content']}\n",
    "\n",
    "        self.savedModelsFolderDict = {1:'Tier 1 Models', 2:'Tier 2 Models'}\n",
    "\n",
    "        self.trainLinkDict = {1:'Tier 1 Data.xlsx', 2:'Tier 2 Data.xlsx'}\n",
    "\n",
    "        self.metricTypeDict = {1:load_metric('matthews_correlation'), 2:None}\n",
    "\n",
    "        self.LRDict = {1:2e-5 , 2:2e-5}\n",
    "\n",
    "        self.coreColumns = ['AuthorID','ReviewerID','Criterion','Course','Comment']\n",
    "        self.tierColumsDict = {1:['CommentCode'], 2:['WritingFormatting', 'MissingContent', 'Incorrect']}\n",
    "\n",
    "        self.client = storage.Client()\n",
    "        \n",
    "        self.robertaTokenizer = None\n",
    "        self.robertaModel = None\n",
    "        self.modelName = None\n",
    "        self.computeMetrics = None\n",
    "        \n",
    "        self.testMode = False\n",
    "        \n",
    "    \n",
    "    def setUpFromArgs(self):\n",
    "        parserNamespace = setUpParser()\n",
    "        self.logLevel = parserNamespace.logging_level\n",
    "        self.vocabularySize = parserNamespace.vocabulary_size\n",
    "        self.hiddenLayers = parserNamespace.hidden_layers\n",
    "        self.tierLevel = parserNamespace.tier_level\n",
    "        self.batchSize = parserNamespace.batch_size\n",
    "        self.metricType = parserNamespace.metric_type\n",
    "        self.numEpochs = parserNamespace.num_epochs\n",
    "        self.earlyStopping = parserNamespace.early_stopping\n",
    "        self.hpTune = parserNamespace.hp_tune\n",
    "        \n",
    "        self.tokenizerBucketName = parserNamespace.tokenizer_bucket\n",
    "        self.modelBucketName = parserNamespace.model_bucket\n",
    "        self.dataBucketName = parserNamespace.data_bucket\n",
    "        \n",
    "            \n",
    "    def loadTokenizer(self):\n",
    "        tokenizerBucket = self.client.get_bucket(self.tokenizerBucketName)\n",
    "\n",
    "        for file in tokenizerBucket.list_blobs(prefix=str(self.vocabularySize)):\n",
    "            if not os.path.exists(os.path.join(self.tempDownloadFolder,str(self.vocabularySize))):\n",
    "                os.makedirs(os.path.join(self.tempDownloadFolder,str(self.vocabularySize)))\n",
    "            if not os.path.exists(os.path.join(self.tempDownloadFolder,file.name)):\n",
    "                file.download_to_filename(os.path.join(self.tempDownloadFolder,file.name))\n",
    "\n",
    "        self.robertaTokenizer = RobertaTokenizer.from_pretrained(os.path.join(self.tempDownloadFolder,str(self.vocabularySize)), do_lower_case=True)\n",
    "        \n",
    "\n",
    "    def loadModel(self):\n",
    "        if self.hiddenLayers > -1:\n",
    "            robertaConfig = RobertaConfig(\n",
    "                                        num_labels = 3,\n",
    "                                        vocab_size=self.vocabularySize,  \n",
    "                                        max_position_embeddings=514, \n",
    "                                        hidden_size=768,\n",
    "                                        num_attention_heads=8, \n",
    "                                        num_hidden_layers=self.hiddenLayers, \n",
    "                                        type_vocab_size=1,\n",
    "                                        problem_type=self.problemTypeDict[self.tierLevel],\n",
    "                                        )\n",
    "\n",
    "            self.robertaModel = RobertaForSequenceClassification(robertaConfig)\n",
    "            self.modelName = 'Model_'+str(self.hiddenLayers)+'_Layers_'+str(self.tierLevel)+'_Tier_'+str(self.vocabularySize)\n",
    "            \n",
    "        else:\n",
    "            self.robertaModel = RobertaForSequenceClassification.from_pretrained(\n",
    "                                                                        'roberta-base',\n",
    "                                                                        num_labels = 3,\n",
    "                                                                        output_attentions = False,\n",
    "                                                                        output_hidden_states = False,\n",
    "                                                                        vocab_size = self.vocabularySize,\n",
    "                                                                        problem_type=self.problemTypeDict[self.tierLevel],\n",
    "                                                                        )\n",
    "            \n",
    "            self.modelName = 'Model_roberta-base_'+str(self.tierLevel)+'_Tier_'+str(self.vocabularySize)\n",
    "            \n",
    "        if self.testMode:\n",
    "            self.modelName = 'test'+self.modelName\n",
    "            \n",
    "    def loadModelFromFile(self):\n",
    "        if self.hiddenLayers > -1:\n",
    "            self.modelName = 'Model_'+str(self.hiddenLayers)+'_Layers_'+str(self.tierLevel)+'_Tier_'+str(self.vocabularySize)\n",
    "        else:\n",
    "            self.modelName = 'Model_roberta-base_'+str(self.tierLevel)+'_Tier_'+str(self.vocabularySize)\n",
    "        if self.testMode:\n",
    "            self.modelName = 'test'+self.modelName\n",
    "            \n",
    "        self.robertaModel = RobertaForSequenceClassification.from_pretrained(os.path.join(self.tempDownloadFolder,self.modelName))\n",
    "        \n",
    "    def setMetrics(self):\n",
    "        return self.metricTypeDict[self.tierLevel]     \n",
    "    \n",
    "    \n",
    "class HPTuneCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A custom callback class that reports a metric to hypertuner\n",
    "    at the end of each epoch.\n",
    "    \"\"\"\n",
    "    def __init__(self, metric_tag, metric_value):\n",
    "        super(HPTuneCallback, self).__init__()\n",
    "        self.metric_tag = metric_tag\n",
    "        self.metric_value = metric_value\n",
    "        self.hpt = hypertune.HyperTune()\n",
    "\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        logging.info(f\"HP metric {self.metric_tag}={kwargs['metrics'][self.metric_value]}\")\n",
    "        self.hpt.report_hyperparameter_tuning_metric(\n",
    "            hyperparameter_metric_tag=self.metric_tag,\n",
    "            metric_value=kwargs['metrics'][self.metric_value],\n",
    "            global_step=state.epoch)\n",
    "\n",
    "    \n",
    "class peerDataset(Dataset):\n",
    "    def __init__(self, df, config):\n",
    "        self.config = config\n",
    "        self.comments = df['Comment'].values \n",
    "        if self.config.tierLevel == 1:\n",
    "            self.labels = torch.tensor(df[self.config.tierColumsDict[self.config.tierLevel]].values,dtype=torch.long)\n",
    "        if self.config.tierLevel == 2:\n",
    "            self.labels = torch.tensor(df[self.config.tierColumsDict[self.config.tierLevel]].values,dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comments)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.tokenizedData = sentenceTokenizer(self.comments[idx], self.config.robertaTokenizer)\n",
    "        self.tokenizedData['labels'] = self.labels[idx]\n",
    "        return self.tokenizedData\n",
    "    \n",
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def setUpParser():\n",
    "    parser = argparse.ArgumentParser(description='Take in command line arguments.')\n",
    "    parser.add_argument(\n",
    "        '--tier-level',\n",
    "        help='Tier level for classification problem. Defaults to 1.',\n",
    "        choices=[1,2],\n",
    "        default=1,\n",
    "        type=int)\n",
    "    parser.add_argument(\n",
    "        '--vocabulary-size',\n",
    "        help='Vocabulary size for Roberta Tokenizer and Models. Defaults to 50265.',\n",
    "        choices=[50265,30522],\n",
    "        default=50265,\n",
    "        type=int)\n",
    "    parser.add_argument(\n",
    "        '--hidden-layers',\n",
    "        help='Number of hidden layers to use if building a model from Roverta Config is used. Set to -1 to use roberta-base instead. Defaults to 6.',\n",
    "        default=6,\n",
    "        type=int)\n",
    "    parser.add_argument(\n",
    "        '--batch-size',\n",
    "        help='Batch size for data fed to the model. Defaults to 16. Warning, higher batch sizes need higher memory requirements, make sure your job is configured appropiately.',\n",
    "        default=16,\n",
    "        type=int)\n",
    "    parser.add_argument(\n",
    "        '--num-epochs',\n",
    "        help='Number of training epochs to run for. Defaults to 100.',\n",
    "        default=100,\n",
    "        type=int)\n",
    "    parser.add_argument(\n",
    "        '--learning-rate',\n",
    "        help='Learning rate value for the optimizers. Defaults to 2e-5.',\n",
    "        default=2e-5,\n",
    "        type=float)\n",
    "    parser.add_argument(\n",
    "        '--metric-type',\n",
    "        help='The metric used in training and validation. Defaults to \"matthews-correlation\". Currently the only one implemented.',\n",
    "        choices=['matthews-correlation'],\n",
    "        default='matthews-correlation',\n",
    "        type=str)\n",
    "    parser.add_argument(\n",
    "        '--tokenizer-bucket',\n",
    "        help='GCP bucket where tokenizer files are stored. Defaults to \"mpr-research-tokenizers\".',\n",
    "        default='mpr-research-tokenizers',\n",
    "        type=str)\n",
    "    parser.add_argument(\n",
    "        '--model-bucket',\n",
    "        help='GCP bucket where model files are uploaded after training. Defaults to \"mpr-research-data\".',\n",
    "        default='mpr-research-models',\n",
    "        type=str)\n",
    "    parser.add_argument(\n",
    "        '--data-bucket',\n",
    "        help='GCP bucket where training files are stored. Defaults to \"mwrite-data-bucket-1\".',\n",
    "        default='mwrite-data-bucket-1',\n",
    "        type=str)\n",
    "    parser.add_argument(\n",
    "        '--seed',\n",
    "        help='Random seed for splitting data. Defaults to 5222.',\n",
    "        default=5222,\n",
    "        type=int)\n",
    "    parser.add_argument(\n",
    "        '--logging-level',\n",
    "        help='Set default Logging Level. Defaults to INFO.',\n",
    "        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'FATAL'],\n",
    "        default='INFO',\n",
    "        type=str)\n",
    "\n",
    "    # Enable hyperparameters\n",
    "    parser.add_argument(\n",
    "        '--early-stopping',\n",
    "        help='Patience for early stopping. Defaults to 3. Set to -1 to disable early stopping.',\n",
    "        default=3,\n",
    "        type=int)\n",
    "    parser.add_argument(\n",
    "        '--hp-tune',\n",
    "        help='Enable hyperparameter tuning. Defaults to \"y\".',\n",
    "        choices=['y','n'],\n",
    "        default='y',\n",
    "        type=str)\n",
    "    parser.add_argument(\n",
    "        '--job-dir',\n",
    "        help='GCS location to export models',\n",
    "        default=os.getenv('AIP_MODEL_DIR'),\n",
    "        type=str)\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def sentenceTokenizer(sentence, tokenizer):\n",
    "    tokenizedSentence = tokenizer(sentence, add_special_tokens = True, \n",
    "                                  max_length= 512, truncation='longest_first', padding='max_length', \n",
    "                                  return_attention_mask=True, return_tensors='pt')\n",
    "    tokenizedSentence = {key:tokenizedSentence[key][0] for key in tokenizedSentence}\n",
    "    return tokenizedSentence\n",
    "\n",
    "\n",
    "def DFToDatasetMaker(config):\n",
    "    df = pd.read_excel('gs://'+os.path.join(config.dataBucketName, config.trainLinkDict[config.tierLevel]))\n",
    "    df = df[config.coreColumns+config.tierColumsDict[config.tierLevel]].dropna()\n",
    "\n",
    "    if config.tierLevel == 1:\n",
    "        trainDF, testDF = train_test_split(df, test_size=0.2, stratify=df[config.tierColumsDict[config.tierLevel]], random_state=5222)\n",
    "        trainDF, valDF = train_test_split(trainDF, test_size=0.25, stratify=trainDF[config.tierColumsDict[config.tierLevel]], random_state=5222)\n",
    "    if config.tierLevel == 2:\n",
    "        dfLow = df[df['MissingContent']==0]\n",
    "        dfHigh = df[(df['MissingContent']==1) & (df['WritingFormatting']==0) & (df['Incorrect']==0)].sample(900)\n",
    "        dfNew = dfHigh.append(dfLow)\n",
    "        trainDF, testDF = train_test_split(dfNew, test_size=0.2, stratify=dfNew[config.tierColumsDict[config.tierLevel]], random_state=5222)\n",
    "        trainDF, valDF = train_test_split(trainDF, test_size=0.25, stratify=trainDF[config.tierColumsDict[config.tierLevel]], random_state=5222)\n",
    "\n",
    "    DFDict = {'train':trainDF, 'test':testDF, 'val':valDF}\n",
    "\n",
    "    dataLoaderDict = {splitType: peerDataset(DFDict[splitType], config) for splitType in DFDict}\n",
    "    \n",
    "    return dataLoaderDict\n",
    "\n",
    "\n",
    "def computeMetricsTier1(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "def computeMetricsTier2(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.where(logits > 0, 1, 0)\n",
    "    adjustFactor = 0.5\n",
    "    mccWeighted = 0.\n",
    "    for index in [0,1,2]:\n",
    "        mccScore = matthews_corrcoef(labels[:,index], np.where(predictions[:,index] > 0, 1, 0))\n",
    "        print(index, mccScore)\n",
    "        mccWeighted += mccScore*(adjustFactor+index)/(3+3*adjustFactor)\n",
    "    return {'Weighted MCC': mccWeighted}\n",
    "    #return {'Hamming Score': 1-hamming_loss(labels, predictions)}\n",
    "\n",
    "computeMetricsDict = {1: computeMetricsTier1,2: computeMetricsTier2}\n",
    "\n",
    "\n",
    "def makeTrainingArgs(config):\n",
    "    return TrainingArguments(\n",
    "                            output_dir=os.path.join(config.tempDownloadFolder,'trainerTier'+str(config.tierLevel)+'Logs'),\n",
    "                            evaluation_strategy='steps',\n",
    "                            logging_steps=100,\n",
    "                            learning_rate=config.LRDict[config.tierLevel],\n",
    "                            per_device_train_batch_size=config.batchSize,\n",
    "                            per_device_eval_batch_size=config.batchSize,\n",
    "                            #auto_find_batch_size = True,\n",
    "                            num_train_epochs=config.numEpochs,\n",
    "                            #label_names=labelTierDict[tierLevel],\n",
    "                            metric_for_best_model = 'eval_loss',\n",
    "                            load_best_model_at_end = True,\n",
    "                            greater_is_better = False\n",
    "                            )\n",
    "\n",
    "\n",
    "def trainerBuilder(config, dataLoaderDict):\n",
    "    trainingArgs = makeTrainingArgs(config)\n",
    "\n",
    "    trainer = Trainer(\n",
    "                    model=config.robertaModel,\n",
    "                    args=trainingArgs,\n",
    "                    train_dataset=dataLoaderDict['train'],\n",
    "                    eval_dataset=dataLoaderDict['val'],\n",
    "                    compute_metrics=computeMetricsDict[config.tierLevel],\n",
    "                    )\n",
    "    \n",
    "    if config.earlyStopping > -1:\n",
    "        trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=config.earlyStopping))\n",
    "    if config.hpTune:\n",
    "        trainer.add_callback(HPTuneCallback(\"loss\", \"eval_loss\"))\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "\n",
    "def getMetrics(testDataLoader, loadedTrainer, config, printMode=False):\n",
    "    predsDict = loadedTrainer.predict(testDataLoader)\n",
    "\n",
    "    truthList = np.array(predsDict.label_ids)\n",
    "\n",
    "    if config.tierLevel == 1:\n",
    "        predList = np.argmax(predsDict.predictions, axis=-1)\n",
    "    if config.tierLevel == 2:\n",
    "        predList = np.where(predsDict.predictions > 0, 1, 0)\n",
    "\n",
    "    if printMode:\n",
    "        logging.info(f\"Macro F1-Score:{f1_score(truthList, predList, average='macro')}\")\n",
    "\n",
    "        if config.tierLevel == 2:\n",
    "            for index,label in enumerate(config.labelTierDict[config.tierLevel]):\n",
    "                logging.info(f'Target Label: {label}')\n",
    "                logging.info(f'MCC: {matthews_corrcoef(truthList[:,index], np.where(predsDict.predictions[:,index] > 0, 1, 0))}')\n",
    "        \n",
    "    return truthList, predList\n",
    "\n",
    "\n",
    "def labelListMaker(dataList, config, index=None):\n",
    "    if config.tierLevel == 1:\n",
    "        return [config.labelTierDict[config.tierLevel][int(value)] for value in dataList]\n",
    "    if config.tierLevel == 2:\n",
    "        labelList = ['No '+config.labelTierDict[config.tierLevel][index], config.labelTierDict[config.tierLevel][index]]\n",
    "        return [labelList[int(value)] for value in dataList]\n",
    "            \n",
    "    \n",
    "def savedModelMetricsViewer(config, dataLoaderDict=None, splitType='test', printMode=False):\n",
    "    \n",
    "    if not dataLoaderDict:\n",
    "        dataLoaderDict = DFToDatasetMaker(config)\n",
    "        \n",
    "    loadedModel = config.robertaModel\n",
    "\n",
    "    loadedTrainer = trainerBuilder(config, dataLoaderDict)\n",
    "\n",
    "    truthList, predList = getMetrics(dataLoaderDict[splitType], loadedTrainer, config, printMode)\n",
    "\n",
    "    if config.tierLevel == 1:\n",
    "        convertedTruthList = labelListMaker(truthList, config)\n",
    "        convertedPredList = labelListMaker(predList, config)\n",
    "\n",
    "        cm = confusion_matrix(convertedTruthList, convertedPredList, labels=config.labelTierDict[config.tierLevel])\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=config.labelTierDict[config.tierLevel])\n",
    "        disp.plot()\n",
    "        plt.show()\n",
    "\n",
    "    if config.tierLevel == 2:\n",
    "        for index,label in enumerate(config.labelTierDict[config.tierLevel]):\n",
    "            convertedTruthList = labelListMaker(truthList[:,index], config, index)\n",
    "            convertedPredList = labelListMaker(predList[:,index], config, index) \n",
    "\n",
    "            labelList = ['No '+config.labelTierDict[config.tierLevel][index], config.labelTierDict[config.tierLevel][index]]\n",
    "            print('Target Label:', label)\n",
    "            cm = confusion_matrix(convertedTruthList, convertedPredList, labels=labelList)\n",
    "            disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labelList)\n",
    "            disp.plot()\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "def saveModelToGCS(config):\n",
    "    try:\n",
    "        modelBucket = config.client.bucket(config.modelBucketName)\n",
    "        localModelPath = os.path.join(config.tempDownloadFolder, config.modelName)\n",
    "        files = [f for f in os.listdir(localModelPath) if os.path.isfile(os.path.join(localModelPath, f))]\n",
    "\n",
    "        for file in files:\n",
    "            localFile = os.path.join(localModelPath, file)\n",
    "            blob = modelBucket.blob(\"/\".join([config.modelName, file]))\n",
    "            blob.upload_from_filename(localFile)\n",
    "        logging.info(f'Saved model files in gs://{config.modelBucketName}/{config.modelName}')\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error: {e}')\n",
    "        logging.error(f'Saved model files instead locally at {os.path.join(config.tempDownloadFolder, config.modelName)}.')\n",
    "        \n",
    "\n",
    "def saveAndUploadModel(trainer, config, cloudSave=True):\n",
    "    \n",
    "    trainer.save_model(os.path.join(config.tempDownloadFolder, config.modelName))\n",
    "    \n",
    "    if cloudSave:\n",
    "        saveModelToGCS(config)\n",
    "    \n",
    "    return True\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d957a77-a2d3-4ad1-8114-07661c7a0aaf",
   "metadata": {},
   "source": [
    "When training in Jupyter, adjust config parameters here before loading in the tokenizer and model.\n",
    "The main one is the tierLevel, which switches which tier level of classification the model is to be trained on\n",
    "The number of hiddern layer lets you make a custom model, set to -1 to use the roberta-base one. \n",
    "testMode just appends 'test' to the start of the saved files. Use it when you are debugging.\n",
    "\n",
    "Simply put, for two levels of classifcation, you need to run the code block twice, adjusting the tierLevel variable for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c49b8f37-91cd-4356-abe6-4f64e5ec4a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file ./tmp/50265/added_tokens.json. We won't load it.\n",
      "Didn't find file ./tmp/50265/special_tokens_map.json. We won't load it.\n",
      "Didn't find file ./tmp/50265/tokenizer_config.json. We won't load it.\n",
      "loading file ./tmp/50265/vocab.json\n",
      "loading file ./tmp/50265/merges.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/jupyter/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2022-06-19T00:47:06+0000 INFO [2487603472.py:11] - Loaded in config for job.\n",
      "2022-06-19T00:47:07+0000 INFO [2487603472.py:15] - Retrieved and set up data batching pipeline.\n",
      "using `logging_steps` to initialize `eval_steps` to 100\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "2022-06-19T00:47:07+0000 INFO [2487603472.py:19] - Trainer set up.\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 3742\n",
      "  Num Epochs = 25\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5850\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='800' max='5850' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 800/5850 09:15 < 58:37, 1.44 it/s, Epoch 3/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Matthews Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.744400</td>\n",
       "      <td>0.684670</td>\n",
       "      <td>0.408544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.689700</td>\n",
       "      <td>0.612098</td>\n",
       "      <td>0.523195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.571100</td>\n",
       "      <td>0.494735</td>\n",
       "      <td>0.612917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.508500</td>\n",
       "      <td>0.475926</td>\n",
       "      <td>0.666784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.418000</td>\n",
       "      <td>0.451936</td>\n",
       "      <td>0.672073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.389500</td>\n",
       "      <td>0.492686</td>\n",
       "      <td>0.694606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.503348</td>\n",
       "      <td>0.664399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.515284</td>\n",
       "      <td>0.693800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1248\n",
      "  Batch size = 16\n",
      "2022-06-19T00:48:16+0000 INFO [3592687769.py:163] - HP metric loss=0.6846699118614197\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1248\n",
      "  Batch size = 16\n",
      "2022-06-19T00:49:24+0000 INFO [3592687769.py:163] - HP metric loss=0.6120984554290771\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1248\n",
      "  Batch size = 16\n",
      "2022-06-19T00:50:31+0000 INFO [3592687769.py:163] - HP metric loss=0.49473464488983154\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1248\n",
      "  Batch size = 16\n",
      "2022-06-19T00:51:39+0000 INFO [3592687769.py:163] - HP metric loss=0.4759259819984436\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1248\n",
      "  Batch size = 16\n",
      "2022-06-19T00:52:47+0000 INFO [3592687769.py:163] - HP metric loss=0.4519355595111847\n",
      "Saving model checkpoint to ./tmp/trainerTier1Logs/checkpoint-500\n",
      "Configuration saved in ./tmp/trainerTier1Logs/checkpoint-500/config.json\n",
      "Model weights saved in ./tmp/trainerTier1Logs/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1248\n",
      "  Batch size = 16\n",
      "2022-06-19T00:54:08+0000 INFO [3592687769.py:163] - HP metric loss=0.4926857054233551\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1248\n",
      "  Batch size = 16\n",
      "2022-06-19T00:55:16+0000 INFO [3592687769.py:163] - HP metric loss=0.5033475756645203\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1248\n",
      "  Batch size = 16\n",
      "2022-06-19T00:56:24+0000 INFO [3592687769.py:163] - HP metric loss=0.5152835249900818\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./tmp/trainerTier1Logs/checkpoint-500 (score: 0.4519355595111847).\n",
      "2022-06-19T00:56:24+0000 INFO [2487603472.py:23] - Training complete.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1248\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='78' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [78/78 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-19T00:56:37+0000 INFO [3592687769.py:163] - HP metric loss=0.47335943579673767\n",
      "2022-06-19T00:56:38+0000 INFO [2487603472.py:28] - Saving model to GCP.\n",
      "Saving model checkpoint to ./tmp/Model_roberta-base_1_Tier_50265\n",
      "Configuration saved in ./tmp/Model_roberta-base_1_Tier_50265/config.json\n",
      "Model weights saved in ./tmp/Model_roberta-base_1_Tier_50265/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configTrain = Config()\n",
    "configTrain.tierLevel = 1\n",
    "configTrain.hiddenLayers = -1\n",
    "configTrain.testMode = False\n",
    "\n",
    "configTrain.loadTokenizer()\n",
    "configTrain.loadModel()\n",
    "global metric\n",
    "metric = configTrain.setMetrics()    \n",
    "\n",
    "logging.info('Loaded in config for job.')\n",
    "\n",
    "dataLoaderDict = DFToDatasetMaker(configTrain)\n",
    "\n",
    "logging.info('Retrieved and set up data batching pipeline.')\n",
    "\n",
    "trainer = trainerBuilder(configTrain, dataLoaderDict)\n",
    "\n",
    "logging.info('Trainer set up.')\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "logging.info('Training complete.')\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=dataLoaderDict['test'])\n",
    "trainer.save_metrics(\"all\", metrics)\n",
    "\n",
    "logging.info('Saving model to GCP.')\n",
    "\n",
    "saveAndUploadModel(trainer, configTrain, True)\n",
    "\n",
    "# Set to False if you do not want to push the model to the GCP bucket. Useful during debugging and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717965ac-a2fd-44ac-910f-9982252a5e21",
   "metadata": {},
   "source": [
    "This is for testing your models after training. tierLevel, hiddenLayers and testMode should let you uniquely idetify any model trained, or saved to file if present. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5546dfd-a8a3-4662-81cb-5dbecc266fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file ./tmp/50265/added_tokens.json. We won't load it.\n",
      "Didn't find file ./tmp/50265/special_tokens_map.json. We won't load it.\n",
      "Didn't find file ./tmp/50265/tokenizer_config.json. We won't load it.\n",
      "loading file ./tmp/50265/vocab.json\n",
      "loading file ./tmp/50265/merges.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file ./tmp/Model_roberta-base_1_Tier_50265/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ./tmp/Model_roberta-base_1_Tier_50265/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ./tmp/Model_roberta-base_1_Tier_50265.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "using `logging_steps` to initialize `eval_steps` to 100\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1248\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='78' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [78/78 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-19T01:00:22+0000 INFO [3592687769.py:374] - Macro F1-Score:0.7733893204603772\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "configTest = Config()\n",
    "configTest.tierLevel = 1\n",
    "configTest.hiddenLayers = -1\n",
    "configTest.testMode = False\n",
    "\n",
    "configTest.loadTokenizer()\n",
    "configTest.loadModelFromFile()\n",
    "\n",
    "savedModelMetricsViewer(configTest,splitType='test',printMode=True)\n",
    "# Set splitmode to train to see training metrics. printMode gives a slightly more verbose output of the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4b8ee0-0967-451e-aa06-d037a27acb75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
